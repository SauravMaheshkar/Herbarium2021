{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "educational-custom",
   "metadata": {
    "papermill": {
     "duration": 0.01442,
     "end_time": "2021-05-09T14:47:17.999735",
     "exception": false,
     "start_time": "2021-05-09T14:47:17.985315",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![](https://github.com/SauravMaheshkar/Herbarium2021/blob/main/assets/Banner.png?raw=true)\n",
    "\n",
    "The Herbarium 2021: Half-Earth Challenge is to identify vascular plant specimens provided by the New York Botanical Garden (NY), Bishop Museum (BPBM), Naturalis Biodiversity Center (NL), Queensland Herbarium (BRI), and Auckland War Memorial Museum (AK).\n",
    "\n",
    "The Herbarium 2021: Half-Earth Challenge dataset includes more than 2.5M images representing nearly 65,000 species from the Americas and Oceania that have been aligned to a standardized plant list (LCVP v1.0.2).\n",
    "\n",
    "This kernel covers how to train a **EfficientNet** using a TFRecords dataset. The notebook is intended to be used on TPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-expense",
   "metadata": {
    "papermill": {
     "duration": 0.012975,
     "end_time": "2021-05-09T14:47:18.026392",
     "exception": false,
     "start_time": "2021-05-09T14:47:18.013417",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id = 'basic'></a>\n",
    "# Packages üì¶ and Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "qualified-secretariat",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-05-09T14:47:18.079540Z",
     "iopub.status.busy": "2021-05-09T14:47:18.067782Z",
     "iopub.status.idle": "2021-05-09T14:47:38.670348Z",
     "shell.execute_reply": "2021-05-09T14:47:38.669654Z"
    },
    "papermill": {
     "duration": 20.63138,
     "end_time": "2021-05-09T14:47:38.670542",
     "exception": false,
     "start_time": "2021-05-09T14:47:18.039162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Install Weights and Biases \n",
    "!pip3 install wandb --upgrade >> /dev/null\n",
    "\n",
    "# Packages\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import re, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import KFold\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "\n",
    "# Configure Logging Level\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "# Weights and Biases Setup\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "wandb.login(key=api_key);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-costa",
   "metadata": {
    "papermill": {
     "duration": 0.012442,
     "end_time": "2021-05-09T14:47:38.696231",
     "exception": false,
     "start_time": "2021-05-09T14:47:38.683789",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Basic Hyperparameters ü™°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "covered-latest",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-09T14:47:38.735216Z",
     "iopub.status.busy": "2021-05-09T14:47:38.734510Z",
     "iopub.status.idle": "2021-05-09T14:47:39.081316Z",
     "shell.execute_reply": "2021-05-09T14:47:39.081862Z"
    },
    "papermill": {
     "duration": 0.37306,
     "end_time": "2021-05-09T14:47:39.082041",
     "exception": false,
     "start_time": "2021-05-09T14:47:38.708981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = \"TPU\" \n",
    "\n",
    "GCS_PATH = KaggleDatasets().get_gcs_path('herb2021-256')\n",
    "\n",
    "IMG_SIZES = 256\n",
    "\n",
    "IMAGE_SIZE = [IMG_SIZES, IMG_SIZES]\n",
    "\n",
    "BATCH_SIZE_SINGLE = 64\n",
    "\n",
    "EPOCHS = 40\n",
    "\n",
    "FOLDS = 10\n",
    "\n",
    "N_CLASSES = 64500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-barbados",
   "metadata": {
    "papermill": {
     "duration": 0.012704,
     "end_time": "2021-05-09T14:47:39.107859",
     "exception": false,
     "start_time": "2021-05-09T14:47:39.095155",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Device Configuration üîå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "coordinate-baghdad",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "execution": {
     "iopub.execute_input": "2021-05-09T14:47:39.209627Z",
     "iopub.status.busy": "2021-05-09T14:47:39.159003Z",
     "iopub.status.idle": "2021-05-09T14:47:44.820693Z",
     "shell.execute_reply": "2021-05-09T14:47:44.820042Z"
    },
    "papermill": {
     "duration": 5.699874,
     "end_time": "2021-05-09T14:47:44.820841",
     "exception": false,
     "start_time": "2021-05-09T14:47:39.120967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting to TPU...\n",
      "Running on TPU  grpc://10.0.0.2:8470\n",
      "initializing  TPU ...\n",
      "TPU initialized\n",
      "REPLICAS: 8\n",
      "BATCH_SIZE: 512\n"
     ]
    }
   ],
   "source": [
    "if DEVICE == \"TPU\":\n",
    "    print(\"connecting to TPU...\")\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        print('Running on TPU ', tpu.master())\n",
    "    except ValueError:\n",
    "        print(\"Could not connect to TPU\")\n",
    "        tpu = None\n",
    "\n",
    "    if tpu:\n",
    "        try:\n",
    "            print(\"initializing  TPU ...\")\n",
    "            tf.config.experimental_connect_to_cluster(tpu)\n",
    "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "            \n",
    "            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "            print(\"TPU initialized\")\n",
    "        except _:\n",
    "            print(\"failed to initialize TPU\")\n",
    "    else:\n",
    "        DEVICE = \"GPU\"\n",
    "\n",
    "if DEVICE == \"GPU\":\n",
    "    n_gpu = len(tf.config.experimental.list_physical_devices('GPU'))\n",
    "    print(\"Num GPUs Available: \", n_gpu)\n",
    "    \n",
    "    if n_gpu > 1:\n",
    "        print(\"Using strategy for multiple GPU\")\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "    else:\n",
    "        print('Standard strategy for GPU...')\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "\n",
    "AUTO     = tf.data.experimental.AUTOTUNE\n",
    "REPLICAS = strategy.num_replicas_in_sync\n",
    "\n",
    "print(f'REPLICAS: {REPLICAS}')\n",
    "\n",
    "BATCH_SIZE = BATCH_SIZE_SINGLE * REPLICAS\n",
    "print(f'BATCH_SIZE: {BATCH_SIZE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-thunder",
   "metadata": {
    "papermill": {
     "duration": 0.013749,
     "end_time": "2021-05-09T14:47:44.849823",
     "exception": false,
     "start_time": "2021-05-09T14:47:44.836074",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id = 'data'></a>\n",
    "# üíø Tensorflow Dataset from TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "powered-sphere",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-05-09T14:47:44.895801Z",
     "iopub.status.busy": "2021-05-09T14:47:44.894749Z",
     "iopub.status.idle": "2021-05-09T14:47:44.913773Z",
     "shell.execute_reply": "2021-05-09T14:47:44.914234Z"
    },
    "papermill": {
     "duration": 0.050858,
     "end_time": "2021-05-09T14:47:44.914425",
     "exception": false,
     "start_time": "2021-05-09T14:47:44.863567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2, seed=12345),\n",
    "])\n",
    "\n",
    "def read_labeled_tfrecord(example):\n",
    "    LABELED_TFREC_FORMAT = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"image_idx\": tf.io.FixedLenFeature([], tf.string),\n",
    "        'label' : tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "    image = decode_image(example['image'])\n",
    "    label = example['label']\n",
    "    return image, label \n",
    "\n",
    "def read_labeled_tfrecord_for_test(example):\n",
    "    LABELED_TFREC_FORMAT = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"image_idx\": tf.io.FixedLenFeature([], tf.string),\n",
    "        'label' : tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "    image = decode_image(example['image'])\n",
    "    label = example['label']\n",
    "        \n",
    "    return image, label \n",
    "\n",
    "def decode_image(image_data):\n",
    "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
    "    image = tf.cast(image, tf.float32)    \n",
    "    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
    "    return image\n",
    "\n",
    "def count_data_items(filenames):\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) \n",
    "         for filename in filenames]\n",
    "    return np.sum(n)\n",
    "\n",
    "def load_dataset(filenames, labeled=True, ordered=False, isTest=False):\n",
    "    ignore_order = tf.data.Options()\n",
    "    \n",
    "    if not ordered:\n",
    "        ignore_order.experimental_deterministic = False\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n",
    "    dataset = dataset.with_options(ignore_order)\n",
    "    \n",
    "    if isTest == False:\n",
    "        dataset = dataset.map(read_labeled_tfrecord)\n",
    "    else:\n",
    "        dataset = dataset.map(read_labeled_tfrecord_for_test)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def get_training_dataset(filenames):\n",
    "    dataset = load_dataset(filenames, labeled=True, isTest = False)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset\n",
    "\n",
    "def get_valid_dataset(filenames):\n",
    "    dataset = load_dataset(filenames, labeled=True, isTest = True)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset\n",
    "\n",
    "def get_test_dataset(filenames):\n",
    "    dataset = load_dataset(filenames, labeled=True, isTest = True, ordered=True)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-plumbing",
   "metadata": {
    "papermill": {
     "duration": 0.013402,
     "end_time": "2021-05-09T14:47:44.941850",
     "exception": false,
     "start_time": "2021-05-09T14:47:44.928448",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# The Model üë∑‚Äç‚ôÄÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-frontier",
   "metadata": {
    "papermill": {
     "duration": 0.01333,
     "end_time": "2021-05-09T14:47:44.968966",
     "exception": false,
     "start_time": "2021-05-09T14:47:44.955636",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Transfer Learning\n",
    "\n",
    "The main aim of transfer learning (TL) is to implement a model quickly i.e. instead of creating a DNN (dense neural network) from scratch, the model will transfer the features it has learned from the different dataset that has performed the same task. This transaction is also known as **knowledge transfer**.\n",
    "\n",
    "---\n",
    "\n",
    "## EfficientNetB4\n",
    "\n",
    "![](https://github.com/SauravMaheshkar/X-Ray-Image-Classification/blob/main/assets/effnet.png?raw=true)\n",
    "\n",
    "> Excerpt from Google AI Blog\n",
    "\n",
    "**Convolutional neural networks (CNNs)** are commonly developed at a fixed resource cost, and then scaled up in order to achieve better accuracy when more resources are made available. For example, ResNet can be scaled up from ResNet-18 to ResNet-200 by increasing the number of layers. The conventional practice for model scaling is to arbitrarily increase the CNN depth or width, or to use larger input image resolution for training and evaluation. While these methods do improve accuracy, they usually require tedious manual tuning, and still often yield suboptimal performance. Instead, the authors of [**\"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (ICML 2019)\"**](https://arxiv.org/abs/1905.11946) found a more principled method to scale up a CNN to obtain better accuracy and efficiency.\n",
    "\n",
    "They proposed a novel model scaling method that uses a simple yet highly effective **compound coefficient** to scale up CNNs in a more structured manner. Unlike conventional approaches that arbitrarily scale network dimensions, such as width, depth and resolution, their method uniformly scales each dimension with a fixed set of scaling coefficients. The resulting models named **EfficientNets**, superpassed state-of-the-art accuracy with up to **10x** better efficiency (**smaller and faster**).\n",
    "\n",
    "In this project we'll use **`EfficientNetB4`** for training our Classifier. The Model can easily be instantiated using the **`tf.keras.applications`** Module, which provides canned architectures with pre-trained weights. For more details kindly visit [this](https://www.tensorflow.org/api_docs/python/tf/keras/applications) link. Unhide the below cell to see the `build_model()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "imposed-biology",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-05-09T14:47:45.011088Z",
     "iopub.status.busy": "2021-05-09T14:47:45.010370Z",
     "iopub.status.idle": "2021-05-09T14:47:45.019977Z",
     "shell.execute_reply": "2021-05-09T14:47:45.019406Z"
    },
    "papermill": {
     "duration": 0.037069,
     "end_time": "2021-05-09T14:47:45.020126",
     "exception": false,
     "start_time": "2021-05-09T14:47:44.983057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tpu_data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\", seed=12345),\n",
    "])\n",
    "\n",
    "def build_model(dim = IMG_SIZES, ef = 0):\n",
    "    inp = tf.keras.layers.Input(shape=(*IMAGE_SIZE, 3))\n",
    "    \n",
    "    base = tf.keras.applications.EfficientNetB3(include_top=False, weights='imagenet', \n",
    "                          input_shape=(*IMAGE_SIZE, 3), pooling='avg')\n",
    "    \n",
    "    x = tpu_data_augmentation(inp)\n",
    "    x = base(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dense(512)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.Dense(N_CLASSES, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs = inp,outputs = x)\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "    \n",
    "    fn_loss = tf.keras.losses.SparseCategoricalCrossentropy() \n",
    "\n",
    "    model.compile(optimizer = opt, loss = [fn_loss], metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "foreign-nitrogen",
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2021-05-09T14:47:45.064186Z",
     "iopub.status.busy": "2021-05-09T14:47:45.058624Z",
     "iopub.status.idle": "2021-05-09T14:47:50.511939Z",
     "shell.execute_reply": "2021-05-09T14:47:50.510833Z"
    },
    "papermill": {
     "duration": 5.477577,
     "end_time": "2021-05-09T14:47:50.512107",
     "exception": false,
     "start_time": "2021-05-09T14:47:45.034530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb3_notop.h5\n",
      "43941888/43941136 [==============================] - 0s 0us/step\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 256, 256, 3)]     0         \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "efficientnetb3 (Functional)  (None, 1536)              10783535  \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 1536)              6144      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64500)             33088500  \n",
      "=================================================================\n",
      "Total params: 44,667,171\n",
      "Trainable params: 44,575,772\n",
      "Non-trainable params: 91,399\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "display_model = build_model(dim=IMG_SIZES)\n",
    "\n",
    "display_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-partnership",
   "metadata": {
    "papermill": {
     "duration": 0.016355,
     "end_time": "2021-05-09T14:47:50.545396",
     "exception": false,
     "start_time": "2021-05-09T14:47:50.529041",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id = 'train'></a>\n",
    "# Training üí™üèª"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-disposal",
   "metadata": {
    "papermill": {
     "duration": 0.016512,
     "end_time": "2021-05-09T14:47:50.578656",
     "exception": false,
     "start_time": "2021-05-09T14:47:50.562144",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LearningRate Scheduler\n",
    "\n",
    "> From a [TowardsDataScience article](https://towardsdatascience.com/learning-rate-scheduler-d8a55747dd90)\n",
    "\n",
    "In training deep networks, it is helpful to reduce the learning rate as the number of training epochs increases. This is **based on the intuition** that with a high learning rate, the deep learning model would possess high kinetic energy. As a result, it‚Äôs parameter vector bounces around chaotically. Thus, it‚Äôs unable to settle down into deeper and narrower parts of the loss function (local minima). If the learning rate, on the other hand, was very small, the system then would have low kinetic energy. Thus, it would settle down into shallow and narrower parts of the loss function (false minima).\n",
    "\n",
    "<center> <img src = \"https://miro.medium.com/max/668/1*iYWyu8hemMyaBlK6V-2vqg.png\"> </center>\n",
    "\n",
    "The above figure depicts that a high learning rate will lead to random to and fro moment of the vector around local minima while a slow learning rate results in getting stuck into false minima. Thus, knowing when to decay the learning rate can be hard to find out.\n",
    "\n",
    "Decreasing the learning rate during training can lead to improved accuracy and (most perplexingly) reduced overfitting of the model. A piecewise decrease of the learning rate whenever progress has plateaued is effective in practice. Essentially this ensures that we converge efficiently to a suitable solution and only then reduce the inherent variance of the parameters by reducing the learning rate.\n",
    "\n",
    "Here, we'll demonstrate how to use LearningRate schedules to automatically **adapt learning rates** that achieve the **optimal rate of convergence** for stochastic gradient descent. Unhide the cell to see the custom callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "golden-phenomenon",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-05-09T14:47:50.619620Z",
     "iopub.status.busy": "2021-05-09T14:47:50.618888Z",
     "iopub.status.idle": "2021-05-09T14:47:50.622175Z",
     "shell.execute_reply": "2021-05-09T14:47:50.621651Z"
    },
    "papermill": {
     "duration": 0.027147,
     "end_time": "2021-05-09T14:47:50.622312",
     "exception": false,
     "start_time": "2021-05-09T14:47:50.595165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_lr_callback(batch_size=8):\n",
    "    lr_start   = 0.0002\n",
    "    lr_max     = 0.0002 * 10\n",
    "    lr_min     = lr_start/2\n",
    "    lr_ramp_ep = 6\n",
    "    lr_sus_ep  = 10\n",
    "    lr_decay   = 0.8\n",
    "   \n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "            \n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max\n",
    "            \n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "            \n",
    "        return lr\n",
    "\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n",
    "    \n",
    "    return lr_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "statistical-blanket",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-09T14:47:50.661138Z",
     "iopub.status.busy": "2021-05-09T14:47:50.660509Z",
     "iopub.status.idle": "2021-05-09T14:47:50.751407Z",
     "shell.execute_reply": "2021-05-09T14:47:50.751914Z"
    },
    "papermill": {
     "duration": 0.112837,
     "end_time": "2021-05-09T14:47:50.752094",
     "exception": false,
     "start_time": "2021-05-09T14:47:50.639257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files for train-validation: 226\n",
      "Total number of image for train-validation: 2257759\n"
     ]
    }
   ],
   "source": [
    "all_files = tf.io.gfile.glob(GCS_PATH + '/train*.tfrec')\n",
    "\n",
    "num_total_files = len(all_files)\n",
    "\n",
    "n_images = count_data_items(all_files)\n",
    "\n",
    "print('Total number of files for train-validation:', num_total_files)\n",
    "print('Total number of image for train-validation:', n_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "confident-taxation",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-05-09T14:47:50.798797Z",
     "iopub.status.busy": "2021-05-09T14:47:50.797841Z",
     "iopub.status.idle": "2021-05-09T14:47:50.801820Z",
     "shell.execute_reply": "2021-05-09T14:47:50.801132Z"
    },
    "papermill": {
     "duration": 0.032552,
     "end_time": "2021-05-09T14:47:50.801959",
     "exception": false,
     "start_time": "2021-05-09T14:47:50.769407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_one_fold(fold, files_train, files_valid):\n",
    "    VERBOSE = 1\n",
    "    tStart = time.time()\n",
    "    \n",
    "    # Better Performance\n",
    "    if DEVICE=='TPU':\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    \n",
    "    # Build the Model\n",
    "    K.clear_session()\n",
    "    with strategy.scope():\n",
    "        print('Building model...')\n",
    "        model = build_model(dim=IMG_SIZES)\n",
    "    \n",
    "    # Callback to Save Model\n",
    "    sv = tf.keras.callbacks.ModelCheckpoint('fold-%i.h5'%fold, monitor='val_loss', verbose=1, save_best_only=True,\n",
    "                                            save_weights_only=True, mode='min', save_freq='epoch')\n",
    "    \n",
    "    # Train for One Fold\n",
    "    history = model.fit(get_training_dataset(files_train), \n",
    "                        epochs=EPOCHS, \n",
    "                        callbacks = [sv, get_lr_callback(BATCH_SIZE), WandbCallback()], \n",
    "                        steps_per_epoch = count_data_items(files_train)/BATCH_SIZE//REPLICAS,\n",
    "                        validation_data = get_valid_dataset(files_valid), \n",
    "                        validation_steps = count_data_items(files_valid)/BATCH_SIZE//REPLICAS,\n",
    "                        verbose=VERBOSE)\n",
    "    \n",
    "    model.save('b3-aug.h5')\n",
    "    \n",
    "    #save it as model artifact on W&B\n",
    "    artifact =  wandb.Artifact(name=\"b3-aug\", type=\"weights\")\n",
    "    artifact.add_file('b3-aug.h5')\n",
    "    wandb.log_artifact(artifact)\n",
    "    \n",
    "    # Record the Time Spent\n",
    "    tElapsed = round(time.time() - tStart, 1)\n",
    "    \n",
    "    print(' ')\n",
    "    print('Time (sec) elapsed: ', tElapsed)\n",
    "    print('...')\n",
    "    print('...')\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "atmospheric-release",
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-05-09T14:47:50.851123Z",
     "iopub.status.busy": "2021-05-09T14:47:50.850409Z",
     "iopub.status.idle": "2021-05-09T17:34:17.378344Z",
     "shell.execute_reply": "2021-05-09T17:34:17.285833Z"
    },
    "papermill": {
     "duration": 9986.558595,
     "end_time": "2021-05-09T17:34:17.378511",
     "exception": false,
     "start_time": "2021-05-09T14:47:50.819916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################################\n",
      "#### FOLD 1\n",
      "#### Epochs: 40\n",
      "############################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msauravmaheshkar\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images 2027759\n",
      "Number of validation images 230000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">unique-spaceship-25</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sauravmaheshkar/Herbarium%202021\" target=\"_blank\">https://wandb.ai/sauravmaheshkar/Herbarium%202021</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/sauravmaheshkar/Herbarium%202021/runs/3eblo2nf\" target=\"_blank\">https://wandb.ai/sauravmaheshkar/Herbarium%202021/runs/3eblo2nf</a><br/>\n",
       "                Run data is saved locally in <code>/kaggle/working/wandb/run-20210509_144807-3eblo2nf</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Epoch 1/40\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "495/495 [==============================] - 376s 591ms/step - loss: 9.9082 - accuracy: 0.0084 - val_loss: 7.6459 - val_accuracy: 0.0638\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 7.64593, saving model to fold-1.h5\n",
      "Epoch 2/40\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "495/495 [==============================] - 288s 582ms/step - loss: 6.8577 - accuracy: 0.1074 - val_loss: 4.8442 - val_accuracy: 0.2636\n",
      "\n",
      "Epoch 00002: val_loss improved from 7.64593 to 4.84418, saving model to fold-1.h5\n",
      "Epoch 3/40\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0007999999999999999.\n",
      "495/495 [==============================] - 271s 548ms/step - loss: 4.5136 - accuracy: 0.2892 - val_loss: 3.8571 - val_accuracy: 0.3490\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.84418 to 3.85706, saving model to fold-1.h5\n",
      "Epoch 4/40\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0011.\n",
      "495/495 [==============================] - 281s 568ms/step - loss: 3.5151 - accuracy: 0.3873 - val_loss: 3.5571 - val_accuracy: 0.3620\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.85706 to 3.55707, saving model to fold-1.h5\n",
      "Epoch 5/40\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0014.\n",
      "495/495 [==============================] - 278s 562ms/step - loss: 3.0141 - accuracy: 0.4432 - val_loss: 2.8651 - val_accuracy: 0.4546\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.55707 to 2.86509, saving model to fold-1.h5\n",
      "Epoch 6/40\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0017.\n",
      "495/495 [==============================] - 256s 517ms/step - loss: 2.6912 - accuracy: 0.4816 - val_loss: 2.5541 - val_accuracy: 0.5023\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.86509 to 2.55408, saving model to fold-1.h5\n",
      "Epoch 7/40\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.002.\n",
      "495/495 [==============================] - 232s 470ms/step - loss: 2.4676 - accuracy: 0.5082 - val_loss: 2.4456 - val_accuracy: 0.5098\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.55408 to 2.44559, saving model to fold-1.h5\n",
      "Epoch 8/40\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.002.\n",
      "495/495 [==============================] - 227s 460ms/step - loss: 2.2262 - accuracy: 0.5480 - val_loss: 2.2703 - val_accuracy: 0.5309\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.44559 to 2.27027, saving model to fold-1.h5\n",
      "Epoch 9/40\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.002.\n",
      "495/495 [==============================] - 230s 464ms/step - loss: 1.9207 - accuracy: 0.5787 - val_loss: 2.3598 - val_accuracy: 0.5182\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.27027\n",
      "Epoch 10/40\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.002.\n",
      "495/495 [==============================] - 236s 478ms/step - loss: 1.7227 - accuracy: 0.6046 - val_loss: 2.3885 - val_accuracy: 0.5110\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.27027\n",
      "Epoch 11/40\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.002.\n",
      "495/495 [==============================] - 236s 477ms/step - loss: 1.4982 - accuracy: 0.6320 - val_loss: 2.7391 - val_accuracy: 0.4728\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.27027\n",
      "Epoch 12/40\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.002.\n",
      "495/495 [==============================] - 236s 476ms/step - loss: 1.3891 - accuracy: 0.6543 - val_loss: 2.2237 - val_accuracy: 0.5417\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.27027 to 2.22373, saving model to fold-1.h5\n",
      "Epoch 13/40\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.002.\n",
      "495/495 [==============================] - 228s 460ms/step - loss: 1.3194 - accuracy: 0.6682 - val_loss: 2.1899 - val_accuracy: 0.5541\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.22373 to 2.18991, saving model to fold-1.h5\n",
      "Epoch 14/40\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.002.\n",
      "495/495 [==============================] - 226s 457ms/step - loss: 1.2590 - accuracy: 0.6824 - val_loss: 2.1741 - val_accuracy: 0.5622\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.18991 to 2.17412, saving model to fold-1.h5\n",
      "Epoch 15/40\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.002.\n",
      "495/495 [==============================] - 229s 462ms/step - loss: 1.1999 - accuracy: 0.6950 - val_loss: 1.9767 - val_accuracy: 0.5939\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.17412 to 1.97668, saving model to fold-1.h5\n",
      "Epoch 16/40\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.002.\n",
      "495/495 [==============================] - 230s 464ms/step - loss: 1.1632 - accuracy: 0.7051 - val_loss: 1.9519 - val_accuracy: 0.5920\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.97668 to 1.95191, saving model to fold-1.h5\n",
      "Epoch 17/40\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.002.\n",
      "495/495 [==============================] - 230s 465ms/step - loss: 1.0642 - accuracy: 0.7212 - val_loss: 2.3028 - val_accuracy: 0.5441\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.95191\n",
      "Epoch 18/40\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0016200000000000001.\n",
      "495/495 [==============================] - 236s 477ms/step - loss: 0.9617 - accuracy: 0.7452 - val_loss: 1.6348 - val_accuracy: 0.6579\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.95191 to 1.63475, saving model to fold-1.h5\n",
      "Epoch 19/40\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0013160000000000003.\n",
      "495/495 [==============================] - 230s 464ms/step - loss: 0.8139 - accuracy: 0.7786 - val_loss: 1.8871 - val_accuracy: 0.6140\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.63475\n",
      "Epoch 20/40\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0010728000000000003.\n",
      "495/495 [==============================] - 234s 474ms/step - loss: 0.7141 - accuracy: 0.8030 - val_loss: 1.5327 - val_accuracy: 0.6865\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.63475 to 1.53267, saving model to fold-1.h5\n",
      "Epoch 21/40\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0008782400000000002.\n",
      "495/495 [==============================] - 230s 464ms/step - loss: 0.6407 - accuracy: 0.8212 - val_loss: 1.4162 - val_accuracy: 0.7073\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.53267 to 1.41617, saving model to fold-1.h5\n",
      "Epoch 22/40\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.0007225920000000002.\n",
      "495/495 [==============================] - 228s 461ms/step - loss: 0.5869 - accuracy: 0.8357 - val_loss: 1.3204 - val_accuracy: 0.7318\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.41617 to 1.32037, saving model to fold-1.h5\n",
      "Epoch 23/40\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0005980736000000003.\n",
      "495/495 [==============================] - 232s 469ms/step - loss: 0.5265 - accuracy: 0.8520 - val_loss: 1.2983 - val_accuracy: 0.7340\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.32037 to 1.29833, saving model to fold-1.h5\n",
      "Epoch 24/40\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0004984588800000002.\n",
      "495/495 [==============================] - 230s 464ms/step - loss: 0.4766 - accuracy: 0.8647 - val_loss: 1.1728 - val_accuracy: 0.7565\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.29833 to 1.17282, saving model to fold-1.h5\n",
      "Epoch 25/40\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.00041876710400000014.\n",
      "495/495 [==============================] - 227s 459ms/step - loss: 0.4175 - accuracy: 0.8819 - val_loss: 1.1938 - val_accuracy: 0.7618\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.17282\n",
      "Epoch 26/40\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0003550136832000001.\n",
      "495/495 [==============================] - 238s 481ms/step - loss: 0.4068 - accuracy: 0.8845 - val_loss: 1.1564 - val_accuracy: 0.7691\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.17282 to 1.15636, saving model to fold-1.h5\n",
      "Epoch 27/40\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0003040109465600001.\n",
      "495/495 [==============================] - 228s 461ms/step - loss: 0.3681 - accuracy: 0.8938 - val_loss: 1.1773 - val_accuracy: 0.7688\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.15636\n",
      "Epoch 28/40\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0002632087572480001.\n",
      "495/495 [==============================] - 235s 476ms/step - loss: 0.3454 - accuracy: 0.9022 - val_loss: 1.1608 - val_accuracy: 0.7743\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.15636\n",
      "Epoch 29/40\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0002305670057984001.\n",
      "495/495 [==============================] - 235s 475ms/step - loss: 0.3338 - accuracy: 0.9036 - val_loss: 1.1520 - val_accuracy: 0.7765\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.15636 to 1.15199, saving model to fold-1.h5\n",
      "Epoch 30/40\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.00020445360463872006.\n",
      "495/495 [==============================] - 225s 455ms/step - loss: 0.3285 - accuracy: 0.9061 - val_loss: 1.1679 - val_accuracy: 0.7720\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.15199\n",
      "Epoch 31/40\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.00018356288371097608.\n",
      "495/495 [==============================] - 236s 476ms/step - loss: 0.3104 - accuracy: 0.9106 - val_loss: 1.1280 - val_accuracy: 0.7827\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.15199 to 1.12797, saving model to fold-1.h5\n",
      "Epoch 32/40\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.00016685030696878087.\n",
      "495/495 [==============================] - 229s 463ms/step - loss: 0.3002 - accuracy: 0.9141 - val_loss: 1.1121 - val_accuracy: 0.7851\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.12797 to 1.11209, saving model to fold-1.h5\n",
      "Epoch 33/40\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.0001534802455750247.\n",
      "495/495 [==============================] - 229s 462ms/step - loss: 0.2780 - accuracy: 0.9206 - val_loss: 1.1241 - val_accuracy: 0.7849\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.11209\n",
      "Epoch 34/40\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.00014278419646001977.\n",
      "495/495 [==============================] - 237s 479ms/step - loss: 0.2832 - accuracy: 0.9190 - val_loss: 1.1172 - val_accuracy: 0.7870\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.11209\n",
      "Epoch 35/40\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.0001342273571680158.\n",
      "495/495 [==============================] - 235s 476ms/step - loss: 0.2681 - accuracy: 0.9227 - val_loss: 1.1475 - val_accuracy: 0.7832\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.11209\n",
      "Epoch 36/40\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.00012738188573441266.\n",
      "495/495 [==============================] - 236s 476ms/step - loss: 0.2582 - accuracy: 0.9260 - val_loss: 1.1385 - val_accuracy: 0.7855\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.11209\n",
      "Epoch 37/40\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.00012190550858753011.\n",
      "495/495 [==============================] - 236s 477ms/step - loss: 0.2557 - accuracy: 0.9265 - val_loss: 1.1331 - val_accuracy: 0.7865\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.11209\n",
      "Epoch 38/40\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.0001175244068700241.\n",
      "495/495 [==============================] - 236s 476ms/step - loss: 0.2588 - accuracy: 0.9248 - val_loss: 1.1394 - val_accuracy: 0.7871\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.11209\n",
      "Epoch 39/40\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.00011401952549601928.\n",
      "495/495 [==============================] - 235s 476ms/step - loss: 0.2498 - accuracy: 0.9286 - val_loss: 1.1383 - val_accuracy: 0.7878\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.11209\n",
      "Epoch 40/40\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.00011121562039681542.\n",
      "495/495 [==============================] - 238s 482ms/step - loss: 0.2495 - accuracy: 0.9290 - val_loss: 1.1247 - val_accuracy: 0.7867\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.11209\n",
      " \n",
      "Time (sec) elapsed:  9928.1\n",
      "...\n",
      "...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 162<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62208a7f30834df2bb6120a3b5154c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 1023.70MB of 1023.70MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, ma‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/kaggle/working/wandb/run-20210509_144807-3eblo2nf/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/kaggle/working/wandb/run-20210509_144807-3eblo2nf/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>39</td></tr><tr><td>loss</td><td>0.24459</td></tr><tr><td>accuracy</td><td>0.93037</td></tr><tr><td>val_loss</td><td>1.12466</td></tr><tr><td>val_accuracy</td><td>0.78669</td></tr><tr><td>lr</td><td>0.00011</td></tr><tr><td>_runtime</td><td>9920</td></tr><tr><td>_timestamp</td><td>1620581607</td></tr><tr><td>_step</td><td>39</td></tr><tr><td>best_val_loss</td><td>1.11209</td></tr><tr><td>best_epoch</td><td>31</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>loss</td><td>‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>accuracy</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val_loss</td><td>‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_accuracy</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>_runtime</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>_timestamp</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">unique-spaceship-25</strong>: <a href=\"https://wandb.ai/sauravmaheshkar/Herbarium%202021/runs/3eblo2nf\" target=\"_blank\">https://wandb.ai/sauravmaheshkar/Herbarium%202021/runs/3eblo2nf</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SHOW_FILES = True\n",
    "\n",
    "STOP_FOLDS = 0\n",
    "\n",
    "skf = KFold(n_splits = FOLDS, shuffle = True, random_state=54321)\n",
    "\n",
    "histories = []\n",
    "\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(np.arange(num_total_files))):\n",
    "    print('')\n",
    "    print('#'*60) \n",
    "    print('#### FOLD', fold+1)\n",
    "    print('#### Epochs: %i' %(EPOCHS))\n",
    "    print('#'*60)\n",
    "    \n",
    "    train_files = tf.io.gfile.glob([GCS_PATH + '/train%.3i*.tfrec'%x for x in idxT])\n",
    "    valid_files = tf.io.gfile.glob([GCS_PATH + '/train%.3i*.tfrec'%x for x in idxV])\n",
    "    \n",
    "    if SHOW_FILES:\n",
    "        print('Number of training images', count_data_items(train_files))\n",
    "        print('Number of validation images', count_data_items(valid_files))\n",
    "        \n",
    "    run = wandb.init(project='Herbarium 2021', entity='sauravmaheshkar', reinit=True)\n",
    "    \n",
    "    history = train_one_fold(fold+1, train_files, valid_files)\n",
    "    \n",
    "    run.finish()\n",
    "    \n",
    "    histories.append(history)\n",
    "\n",
    "    if fold >= STOP_FOLDS:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10038.408335,
   "end_time": "2021-05-09T17:34:28.534742",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-09T14:47:10.126407",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "62208a7f30834df2bb6120a3b5154c34": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "VBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "VBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_eb1329760e1b4d47b0d1464c158e1192",
        "IPY_MODEL_dc2ea25db5994711baaa1b9fa2cd7ce0"
       ],
       "layout": "IPY_MODEL_dce50a1ad1964f969b6ac6447247bfc6"
      }
     },
     "6cec288f026c4adf9d870ea2c75abc76": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7f864bbf11b645819247d384e6783e98": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ad384642805f4c889b81126e6277e3ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "be9541e8cc7e4dbd8fe5cb6a705e2119": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dc2ea25db5994711baaa1b9fa2cd7ce0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6cec288f026c4adf9d870ea2c75abc76",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7f864bbf11b645819247d384e6783e98",
       "value": 1.0
      }
     },
     "dce50a1ad1964f969b6ac6447247bfc6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eb1329760e1b4d47b0d1464c158e1192": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "LabelModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "LabelModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "LabelView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_be9541e8cc7e4dbd8fe5cb6a705e2119",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_ad384642805f4c889b81126e6277e3ef",
       "value": " 1023.71MB of 1023.71MB uploaded (0.00MB deduped)\r"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
